{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6fa8fe",
   "metadata": {},
   "source": [
    "# Assignment 5 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde2d6b",
   "metadata": {},
   "source": [
    "Zach Novak, Marco Bogani, Sulaiman Karmali, Daman Sawhney, Ivan Lima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from nltk import download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "190f358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e66c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "download('punkt')\n",
    "download('wordnet')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3056adb",
   "metadata": {},
   "source": [
    "1. Load the provided dataset containing financial news headlines and sentiment labels. Perform exploratory data analysis to understand the structure of the dataset, distribution of sentiment labels, and any other relevant insights. ( 5 points )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1022bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Loughran-McDonald Master Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b449d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Loughran-McDonald_MasterDictionary_1993-2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68887170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CNBC headlines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnbc_data = pd.read_csv('cnbc_headlines2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707fa5b8",
   "metadata": {},
   "source": [
    "2. Clean the text data by removing punctuation, special characters, and irrelevant symbols. Tokenize the headlines and convert them to lowercase for uniformity. Implement techniques like stemming or lemmatization to normalize the text data. ( 5 points )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in 'Headlines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124686a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnbc_data.dropna(subset=['Description'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f873157",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = data[data['Positive'] > 0]['Word'].str.lower().unique()\n",
    "negative_words = data[data['Negative'] > 0]['Word'].str.lower().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ee7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to analyze sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(description):\n",
    "    tokens = word_tokenize(str(description).lower())\n",
    "    positive_count = sum(token in positive_words for token in tokens)\n",
    "    negative_count = sum(token in negative_words for token in tokens)\n",
    "    if positive_count > negative_count:\n",
    "        return 'Positive'\n",
    "    elif negative_count > positive_count:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679dd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lowercase, lemmatize, and remove stopwords\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n",
    "    return ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa305a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9acd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnbc_data['Cleaned_Description'] = cnbc_data['Description'].apply(clean_text)\n",
    "cnbc_data['Sentiment'] = cnbc_data['Cleaned_Description'].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf2c02",
   "metadata": {},
   "source": [
    "3. Convert the text data into numerical features suitable for machine learning models. You can use techniques like bag-of-words, TF-IDF, or word embeddings. Split the dataset into training and testing sets. ( 5 points )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527eb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data into numerical features using the 'bag-of-words' approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "features = vectorizer.fit_transform(cnbc_data['Cleaned_Description']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ffaa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment labels to numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39add4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {'Positive': 1, 'Negative': -1, 'Neutral': 0}\n",
    "labels = cnbc_data['Sentiment'].map(sentiment_mapping).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c7e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad85527",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d6138",
   "metadata": {},
   "source": [
    "4. Choose appropriate machine learning algorithms (e.g., Naive Bayes, Support Vector Machines, or Neural Networks) for sentiment analysis. Train the model using the training data and evaluate its performance using appropriate evaluation metrics (accuracy, precision, recall, F1-score). ( 5 points )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 'Naive Bayes' to train the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef2d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61722c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d263c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructed the vocabulary of the bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n**Vocabulary of the bag-of-words model**\")\n",
    "print(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5281adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n**Index positions of vocabulary**\")\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195620be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform exploratory data analysis on the sentiment distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e40b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n**Sentiment Count**\")\n",
    "sentiment_counts = cnbc_data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bdb9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiment_counts)\n",
    "print(\"\\n**Performance**\")\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
